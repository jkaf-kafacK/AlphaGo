{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Value Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Value Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERBOSE True\n",
      ">> forward >>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>> (torch.Size([512, 1, 9, 9]), torch.float32)\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "  >> conv1   >>> torch.Size([512, 32, 9, 9])\n",
      "  >> batch   >>> torch.Size([512, 32, 9, 9])\n",
      "  >> relu   >>> torch.Size([512, 32, 9, 9])\n",
      "  >> conv2   >>> torch.Size([512, 64, 9, 9])\n",
      "  >> batch   >>> torch.Size([512, 64, 9, 9])\n",
      "  >> relu   >>> torch.Size([512, 64, 9, 9])\n",
      "  >> conv3   >>> torch.Size([512, 128, 9, 9])\n",
      "  >> batch   >>> torch.Size([512, 128, 9, 9])\n",
      "  >> relu   >>> torch.Size([512, 128, 9, 9])\n",
      "  >> flatten   >>> torch.Size([512, 10368])\n",
      "  >> fc1   >>> torch.Size([512, 512])\n",
      "  >> relu   >>> torch.Size([512, 512])\n",
      "  >> compute output   >>> policy: torch.Size([512, 82])   >>> value: torch.Size([512, 1])\n",
      ">> forward complete >>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0120, 0.0121, 0.0147,  ..., 0.0091, 0.0117, 0.0118],\n",
       "         [0.0118, 0.0118, 0.0145,  ..., 0.0093, 0.0119, 0.0117],\n",
       "         [0.0116, 0.0115, 0.0157,  ..., 0.0105, 0.0134, 0.0108],\n",
       "         ...,\n",
       "         [0.0096, 0.0114, 0.0143,  ..., 0.0084, 0.0133, 0.0135],\n",
       "         [0.0105, 0.0111, 0.0158,  ..., 0.0092, 0.0143, 0.0138],\n",
       "         [0.0107, 0.0118, 0.0157,  ..., 0.0092, 0.0150, 0.0138]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[ 8.2279e-02],\n",
       "         [ 8.0492e-02],\n",
       "         [ 1.2844e-01],\n",
       "         [ 9.5193e-02],\n",
       "         [ 7.6025e-02],\n",
       "         [ 5.2270e-02],\n",
       "         [-8.8554e-02],\n",
       "         [-3.9973e-02],\n",
       "         [-3.0436e-02],\n",
       "         [-4.7412e-02],\n",
       "         [-8.2306e-04],\n",
       "         [ 1.4189e-02],\n",
       "         [ 1.5434e-01],\n",
       "         [ 1.1350e-01],\n",
       "         [ 5.6272e-02],\n",
       "         [ 3.3449e-02],\n",
       "         [-2.2195e-03],\n",
       "         [ 2.7218e-02],\n",
       "         [-4.4499e-02],\n",
       "         [-7.2663e-03],\n",
       "         [-1.7305e-03],\n",
       "         [-5.1940e-02],\n",
       "         [-7.3804e-02],\n",
       "         [ 2.5201e-03],\n",
       "         [ 5.2523e-02],\n",
       "         [ 5.1498e-02],\n",
       "         [ 7.8643e-02],\n",
       "         [ 8.9438e-02],\n",
       "         [ 1.3781e-02],\n",
       "         [-2.0140e-02],\n",
       "         [-9.3192e-02],\n",
       "         [-6.0281e-02],\n",
       "         [ 1.2395e-02],\n",
       "         [ 1.2254e-01],\n",
       "         [ 3.6151e-01],\n",
       "         [ 3.0263e-01],\n",
       "         [ 3.1065e-01],\n",
       "         [ 2.7251e-01],\n",
       "         [ 2.4471e-01],\n",
       "         [ 2.1663e-01],\n",
       "         [ 2.8989e-01],\n",
       "         [ 8.2279e-02],\n",
       "         [ 8.0492e-02],\n",
       "         [ 6.1507e-02],\n",
       "         [ 1.7777e-02],\n",
       "         [ 6.4981e-02],\n",
       "         [-1.5452e-02],\n",
       "         [ 5.1413e-02],\n",
       "         [ 2.6180e-02],\n",
       "         [-4.4532e-02],\n",
       "         [-1.0015e-01],\n",
       "         [ 4.0693e-03],\n",
       "         [-7.0796e-03],\n",
       "         [ 2.0579e-02],\n",
       "         [-1.2235e-02],\n",
       "         [ 1.1435e-02],\n",
       "         [-6.8919e-02],\n",
       "         [-1.2902e-02],\n",
       "         [-6.6670e-02],\n",
       "         [-8.0706e-03],\n",
       "         [-5.1898e-03],\n",
       "         [-6.4808e-02],\n",
       "         [-5.3469e-02],\n",
       "         [ 7.8049e-03],\n",
       "         [-3.6707e-02],\n",
       "         [-4.0931e-02],\n",
       "         [ 8.2279e-02],\n",
       "         [ 4.1739e-02],\n",
       "         [ 2.2044e-02],\n",
       "         [ 2.0023e-02],\n",
       "         [-3.7793e-02],\n",
       "         [-1.8168e-02],\n",
       "         [-9.6882e-03],\n",
       "         [-1.7900e-02],\n",
       "         [ 1.1527e-01],\n",
       "         [ 1.2231e-01],\n",
       "         [ 8.4313e-02],\n",
       "         [ 3.6589e-02],\n",
       "         [ 3.7500e-02],\n",
       "         [ 2.2655e-02],\n",
       "         [-3.0334e-02],\n",
       "         [-3.1480e-02],\n",
       "         [ 2.0812e-02],\n",
       "         [ 4.3180e-02],\n",
       "         [ 9.4145e-02],\n",
       "         [ 4.7023e-02],\n",
       "         [ 1.0037e-02],\n",
       "         [ 1.7720e-02],\n",
       "         [ 2.8008e-02],\n",
       "         [ 4.1878e-02],\n",
       "         [-8.1120e-02],\n",
       "         [-9.7411e-02],\n",
       "         [-1.3720e-01],\n",
       "         [-7.9933e-02],\n",
       "         [-1.5351e-01],\n",
       "         [-1.3239e-01],\n",
       "         [-8.4646e-02],\n",
       "         [-4.8634e-02],\n",
       "         [-1.2397e-01],\n",
       "         [-1.2660e-01],\n",
       "         [-7.9692e-02],\n",
       "         [-7.6338e-02],\n",
       "         [-5.2976e-02],\n",
       "         [-7.8577e-02],\n",
       "         [ 8.2279e-02],\n",
       "         [ 2.3475e-02],\n",
       "         [-6.4786e-02],\n",
       "         [-7.5927e-02],\n",
       "         [-6.5685e-02],\n",
       "         [-4.8773e-02],\n",
       "         [-3.8573e-02],\n",
       "         [-2.6500e-02],\n",
       "         [-7.9488e-02],\n",
       "         [-1.8985e-02],\n",
       "         [ 8.6697e-02],\n",
       "         [ 4.8562e-02],\n",
       "         [ 8.2279e-02],\n",
       "         [ 1.5076e-01],\n",
       "         [ 8.6033e-02],\n",
       "         [ 4.3434e-02],\n",
       "         [ 9.9006e-02],\n",
       "         [ 6.7754e-02],\n",
       "         [-1.5176e-02],\n",
       "         [ 3.5539e-02],\n",
       "         [-3.5042e-02],\n",
       "         [-7.6105e-02],\n",
       "         [-3.4195e-02],\n",
       "         [ 1.8278e-03],\n",
       "         [-2.0239e-02],\n",
       "         [-2.3374e-02],\n",
       "         [-9.9240e-02],\n",
       "         [-4.1610e-02],\n",
       "         [-8.0143e-02],\n",
       "         [-5.5571e-02],\n",
       "         [-7.4236e-02],\n",
       "         [ 1.1844e-02],\n",
       "         [ 5.1660e-02],\n",
       "         [-3.3456e-02],\n",
       "         [-4.7054e-03],\n",
       "         [-7.9653e-03],\n",
       "         [ 6.5480e-03],\n",
       "         [-3.4339e-02],\n",
       "         [-7.9934e-02],\n",
       "         [ 8.2279e-02],\n",
       "         [ 1.1017e-01],\n",
       "         [ 1.0902e-01],\n",
       "         [ 6.0712e-02],\n",
       "         [ 5.3432e-02],\n",
       "         [-6.4039e-03],\n",
       "         [ 9.6476e-02],\n",
       "         [ 8.1285e-02],\n",
       "         [ 3.2407e-02],\n",
       "         [ 7.5233e-02],\n",
       "         [ 1.3776e-01],\n",
       "         [ 7.7263e-02],\n",
       "         [ 1.2276e-01],\n",
       "         [ 7.6641e-02],\n",
       "         [ 1.6759e-01],\n",
       "         [ 8.2279e-02],\n",
       "         [ 8.0492e-02],\n",
       "         [ 5.6793e-02],\n",
       "         [ 2.1787e-02],\n",
       "         [-7.0743e-03],\n",
       "         [ 6.2328e-02],\n",
       "         [ 1.3032e-01],\n",
       "         [ 1.7559e-01],\n",
       "         [ 8.2279e-02],\n",
       "         [ 5.3364e-02],\n",
       "         [ 1.0657e-01],\n",
       "         [ 9.6877e-02],\n",
       "         [ 4.8103e-02],\n",
       "         [ 9.3525e-02],\n",
       "         [ 9.6992e-02],\n",
       "         [ 1.1252e-01],\n",
       "         [-3.4968e-03],\n",
       "         [ 4.3964e-02],\n",
       "         [ 4.6402e-02],\n",
       "         [-2.9010e-02],\n",
       "         [-2.8982e-02],\n",
       "         [ 1.3967e-02],\n",
       "         [ 1.0560e-01],\n",
       "         [ 1.1290e-01],\n",
       "         [ 1.3434e-01],\n",
       "         [ 1.5318e-01],\n",
       "         [ 1.2880e-01],\n",
       "         [ 1.4837e-01],\n",
       "         [ 1.3954e-01],\n",
       "         [ 1.2119e-01],\n",
       "         [ 8.5914e-02],\n",
       "         [ 6.6655e-02],\n",
       "         [ 1.2926e-01],\n",
       "         [ 1.1875e-01],\n",
       "         [ 1.1731e-01],\n",
       "         [ 9.9801e-02],\n",
       "         [-2.0148e-02],\n",
       "         [-2.7702e-02],\n",
       "         [ 8.2279e-02],\n",
       "         [ 4.1978e-02],\n",
       "         [ 1.6118e-01],\n",
       "         [ 1.7830e-01],\n",
       "         [ 1.9531e-01],\n",
       "         [ 2.2335e-01],\n",
       "         [ 1.6963e-01],\n",
       "         [ 1.4902e-01],\n",
       "         [ 9.6824e-02],\n",
       "         [ 6.5236e-02],\n",
       "         [ 8.0302e-02],\n",
       "         [ 1.2173e-01],\n",
       "         [ 5.4963e-02],\n",
       "         [ 7.9660e-03],\n",
       "         [ 4.1656e-02],\n",
       "         [ 8.2905e-02],\n",
       "         [ 2.4165e-02],\n",
       "         [-7.0050e-03],\n",
       "         [-4.2632e-02],\n",
       "         [-6.2372e-02],\n",
       "         [-1.4631e-01],\n",
       "         [-1.3748e-01],\n",
       "         [-2.0110e-01],\n",
       "         [-2.5231e-01],\n",
       "         [-2.0710e-01],\n",
       "         [-1.5464e-01],\n",
       "         [-1.7792e-01],\n",
       "         [-1.8216e-01],\n",
       "         [-1.4141e-01],\n",
       "         [-2.1510e-01],\n",
       "         [-3.0670e-01],\n",
       "         [-2.2994e-01],\n",
       "         [-2.4629e-01],\n",
       "         [ 8.2279e-02],\n",
       "         [ 8.0492e-02],\n",
       "         [ 6.1507e-02],\n",
       "         [ 1.1627e-01],\n",
       "         [ 8.1264e-02],\n",
       "         [ 1.4421e-02],\n",
       "         [ 1.1290e-01],\n",
       "         [ 1.0199e-01],\n",
       "         [ 8.4257e-02],\n",
       "         [ 9.5594e-02],\n",
       "         [ 1.0554e-01],\n",
       "         [ 8.3335e-02],\n",
       "         [ 9.3712e-02],\n",
       "         [ 9.6940e-02],\n",
       "         [ 8.1494e-02],\n",
       "         [ 6.2260e-02],\n",
       "         [ 1.3785e-01],\n",
       "         [ 1.0802e-01],\n",
       "         [ 1.2745e-01],\n",
       "         [ 8.0916e-02],\n",
       "         [ 1.4479e-01],\n",
       "         [ 1.7555e-01],\n",
       "         [ 5.3463e-02],\n",
       "         [ 2.7710e-02],\n",
       "         [ 2.3394e-02],\n",
       "         [ 4.0860e-02],\n",
       "         [ 1.2094e-01],\n",
       "         [ 1.2775e-01],\n",
       "         [ 1.7170e-01],\n",
       "         [ 1.8910e-01],\n",
       "         [ 8.2279e-02],\n",
       "         [ 1.5076e-01],\n",
       "         [ 1.5698e-01],\n",
       "         [ 1.5539e-01],\n",
       "         [ 1.1045e-01],\n",
       "         [ 1.0904e-01],\n",
       "         [-3.6150e-02],\n",
       "         [ 5.1543e-02],\n",
       "         [-8.1911e-02],\n",
       "         [-7.0618e-02],\n",
       "         [ 3.3907e-02],\n",
       "         [ 8.2279e-02],\n",
       "         [ 4.1978e-02],\n",
       "         [ 1.7194e-01],\n",
       "         [ 1.6000e-01],\n",
       "         [ 9.1584e-02],\n",
       "         [ 8.0236e-02],\n",
       "         [ 3.4297e-02],\n",
       "         [ 2.8967e-02],\n",
       "         [ 1.9086e-02],\n",
       "         [-4.3461e-02],\n",
       "         [ 8.6983e-02],\n",
       "         [ 4.6662e-02],\n",
       "         [ 5.4424e-02],\n",
       "         [ 8.0809e-02],\n",
       "         [ 1.1843e-01],\n",
       "         [ 3.4742e-02],\n",
       "         [-2.0537e-02],\n",
       "         [-4.9746e-02],\n",
       "         [-3.0635e-02],\n",
       "         [-7.8550e-02],\n",
       "         [-1.5885e-01],\n",
       "         [-1.4182e-01],\n",
       "         [-6.8243e-02],\n",
       "         [-5.9420e-02],\n",
       "         [ 3.5397e-04],\n",
       "         [ 1.9323e-02],\n",
       "         [-4.2243e-02],\n",
       "         [-7.8985e-02],\n",
       "         [-4.9318e-02],\n",
       "         [-5.9027e-02],\n",
       "         [-2.8446e-02],\n",
       "         [-1.6353e-02],\n",
       "         [-1.2546e-02],\n",
       "         [-2.3594e-02],\n",
       "         [-2.4711e-02],\n",
       "         [ 6.4815e-02],\n",
       "         [ 8.3935e-02],\n",
       "         [-1.7736e-02],\n",
       "         [-5.6075e-03],\n",
       "         [-1.9537e-02],\n",
       "         [-8.7665e-02],\n",
       "         [-1.1828e-01],\n",
       "         [-1.0323e-01],\n",
       "         [-9.2875e-02],\n",
       "         [ 8.2279e-02],\n",
       "         [ 2.3475e-02],\n",
       "         [ 1.6652e-01],\n",
       "         [ 1.7244e-01],\n",
       "         [ 1.8206e-01],\n",
       "         [ 1.2984e-01],\n",
       "         [ 9.1311e-02],\n",
       "         [ 7.9761e-02],\n",
       "         [ 8.4599e-02],\n",
       "         [ 1.2709e-01],\n",
       "         [ 2.2372e-01],\n",
       "         [ 1.6413e-01],\n",
       "         [ 1.2958e-01],\n",
       "         [ 1.3781e-01],\n",
       "         [ 1.4813e-01],\n",
       "         [ 1.1623e-01],\n",
       "         [-9.0970e-02],\n",
       "         [-8.0603e-02],\n",
       "         [-2.8413e-02],\n",
       "         [ 8.2279e-02],\n",
       "         [ 1.1017e-01],\n",
       "         [ 4.7550e-02],\n",
       "         [ 1.1699e-01],\n",
       "         [ 1.1179e-01],\n",
       "         [ 1.3115e-01],\n",
       "         [ 1.9090e-01],\n",
       "         [ 1.7173e-01],\n",
       "         [ 1.4495e-01],\n",
       "         [ 1.3006e-01],\n",
       "         [ 9.0389e-02],\n",
       "         [ 3.3038e-02],\n",
       "         [ 1.1080e-01],\n",
       "         [ 9.1781e-02],\n",
       "         [ 1.0511e-01],\n",
       "         [ 8.4498e-02],\n",
       "         [-2.8206e-02],\n",
       "         [-2.7583e-02],\n",
       "         [-6.2387e-02],\n",
       "         [-3.7040e-02],\n",
       "         [-4.5420e-02],\n",
       "         [-1.6441e-02],\n",
       "         [-3.9991e-02],\n",
       "         [-5.1161e-02],\n",
       "         [ 8.2279e-02],\n",
       "         [ 4.1739e-02],\n",
       "         [ 9.3641e-03],\n",
       "         [-6.8122e-02],\n",
       "         [-8.0090e-05],\n",
       "         [ 2.0208e-03],\n",
       "         [-1.2937e-02],\n",
       "         [ 6.9992e-04],\n",
       "         [ 3.2538e-02],\n",
       "         [ 6.8895e-02],\n",
       "         [ 1.0286e-01],\n",
       "         [ 8.3139e-02],\n",
       "         [ 4.2316e-02],\n",
       "         [ 6.7501e-02],\n",
       "         [ 1.1476e-01],\n",
       "         [ 1.5789e-01],\n",
       "         [ 1.2446e-01],\n",
       "         [ 1.1974e-01],\n",
       "         [ 2.5422e-02],\n",
       "         [ 1.1207e-03],\n",
       "         [ 8.2279e-02],\n",
       "         [ 4.1978e-02],\n",
       "         [ 1.7194e-01],\n",
       "         [ 1.4400e-01],\n",
       "         [ 1.7679e-01],\n",
       "         [ 1.8607e-01],\n",
       "         [ 1.6479e-01],\n",
       "         [ 1.4186e-01],\n",
       "         [ 1.6489e-01],\n",
       "         [ 1.4874e-01],\n",
       "         [ 1.3441e-01],\n",
       "         [ 6.7372e-02],\n",
       "         [-1.1172e-02],\n",
       "         [ 4.7247e-02],\n",
       "         [ 7.5403e-02],\n",
       "         [ 5.7570e-02],\n",
       "         [ 5.6173e-03],\n",
       "         [ 2.6887e-04],\n",
       "         [-3.6933e-02],\n",
       "         [-4.6278e-02],\n",
       "         [-8.7651e-02],\n",
       "         [-1.0994e-01],\n",
       "         [-8.6219e-03],\n",
       "         [-1.4591e-02],\n",
       "         [-6.1929e-03],\n",
       "         [-3.6144e-03],\n",
       "         [ 7.2792e-02],\n",
       "         [ 8.6846e-02],\n",
       "         [ 1.3505e-01],\n",
       "         [ 1.0724e-01],\n",
       "         [ 8.2279e-02],\n",
       "         [ 5.3364e-02],\n",
       "         [ 1.2631e-02],\n",
       "         [ 5.9095e-02],\n",
       "         [ 6.7138e-02],\n",
       "         [ 5.6705e-02],\n",
       "         [ 8.2279e-02],\n",
       "         [ 1.5076e-01],\n",
       "         [ 2.8176e-01],\n",
       "         [ 2.8181e-01],\n",
       "         [ 2.8068e-01],\n",
       "         [ 2.4036e-01],\n",
       "         [ 2.1682e-01],\n",
       "         [ 2.2950e-01],\n",
       "         [ 1.9740e-01],\n",
       "         [ 1.9285e-01],\n",
       "         [ 2.1275e-01],\n",
       "         [ 2.1363e-01],\n",
       "         [ 2.7012e-01],\n",
       "         [ 2.7948e-01],\n",
       "         [ 8.2279e-02],\n",
       "         [ 1.2203e-01],\n",
       "         [ 1.6894e-01],\n",
       "         [ 1.6603e-01],\n",
       "         [ 1.5410e-01],\n",
       "         [ 1.2963e-01],\n",
       "         [ 1.2214e-01],\n",
       "         [ 1.4005e-01],\n",
       "         [ 1.3300e-01],\n",
       "         [ 1.6160e-01],\n",
       "         [ 1.6715e-01],\n",
       "         [ 1.3479e-01],\n",
       "         [ 1.4761e-01],\n",
       "         [ 1.2611e-01],\n",
       "         [ 8.9764e-02],\n",
       "         [ 5.0697e-02],\n",
       "         [ 1.7533e-01],\n",
       "         [ 2.0064e-01],\n",
       "         [ 2.3026e-01],\n",
       "         [ 1.9526e-01],\n",
       "         [ 1.4902e-01],\n",
       "         [ 6.2497e-02],\n",
       "         [ 2.1485e-01],\n",
       "         [ 2.1861e-01],\n",
       "         [ 2.1324e-01],\n",
       "         [ 2.4561e-01],\n",
       "         [ 2.9524e-01],\n",
       "         [ 2.3940e-01],\n",
       "         [ 1.3821e-01],\n",
       "         [ 1.6057e-01],\n",
       "         [ 2.1895e-01],\n",
       "         [ 2.1655e-01],\n",
       "         [ 2.3070e-01],\n",
       "         [ 2.8023e-01],\n",
       "         [ 8.2279e-02],\n",
       "         [ 4.1978e-02],\n",
       "         [-2.2613e-02],\n",
       "         [-4.7827e-02],\n",
       "         [ 6.9051e-02],\n",
       "         [ 3.6373e-02],\n",
       "         [ 6.9040e-03],\n",
       "         [-1.6084e-04],\n",
       "         [-2.4327e-02],\n",
       "         [-2.5875e-02],\n",
       "         [ 1.1851e-01],\n",
       "         [ 8.2682e-02],\n",
       "         [ 3.4613e-02],\n",
       "         [-3.1338e-04],\n",
       "         [-1.8080e-02],\n",
       "         [ 7.7742e-03],\n",
       "         [ 5.6318e-02],\n",
       "         [ 5.2514e-02],\n",
       "         [ 8.2780e-03],\n",
       "         [ 1.9544e-02],\n",
       "         [-8.8986e-03],\n",
       "         [ 3.6364e-02],\n",
       "         [ 8.2279e-02],\n",
       "         [ 4.1978e-02],\n",
       "         [ 1.0609e-01],\n",
       "         [ 6.7671e-02],\n",
       "         [ 1.8839e-02],\n",
       "         [ 4.5394e-02],\n",
       "         [ 1.5193e-02],\n",
       "         [ 1.3775e-02],\n",
       "         [ 4.3289e-02],\n",
       "         [ 8.5486e-02],\n",
       "         [ 1.0723e-01],\n",
       "         [ 1.1406e-01],\n",
       "         [ 7.4530e-02],\n",
       "         [ 1.1855e-02],\n",
       "         [ 4.7800e-02],\n",
       "         [ 6.0424e-02],\n",
       "         [ 5.2134e-02],\n",
       "         [ 8.2279e-02],\n",
       "         [ 4.1739e-02],\n",
       "         [ 3.9380e-02],\n",
       "         [ 5.5905e-02],\n",
       "         [ 1.3536e-01],\n",
       "         [ 1.3727e-01],\n",
       "         [ 7.9484e-02],\n",
       "         [ 3.7296e-02],\n",
       "         [ 2.0285e-01],\n",
       "         [ 2.7068e-01],\n",
       "         [ 2.3909e-01],\n",
       "         [ 2.3790e-01]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class PolicyValueNetwork(nn.Module):\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, kernel, padding=0):\n",
    "        super(PolicyValueNetwork, self).__init__()\n",
    "        self.VERBOSE = False\n",
    "\n",
    "        # Define the convolutional and batch normalisation layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=kernel , padding=padding)\n",
    "        self.bnorm1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=kernel , padding=padding)\n",
    "        self.bnorm2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=kernel , padding=padding)\n",
    "        self.bnorm3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Define the fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 9 * 9, 512)\n",
    "        self.fc_policy = nn.Linear(512, 9*9+1)\n",
    "        self.fc_value = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        VERBOSE = self.VERBOSE\n",
    "        if VERBOSE: print('VERBOSE',VERBOSE)\n",
    "        if VERBOSE: print(f'>> forward >>>>>>>>>>>>>>>>>>')\n",
    "        if VERBOSE: print(f'>>>>>>>>>> {x.shape, x.dtype}')\n",
    "        if VERBOSE: print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "\n",
    "        # Pass the input through the convolutional layers\n",
    "        if VERBOSE: print(f'  >> conv1', end=' ')\n",
    "        x = self.conv1(x)\n",
    "        if VERBOSE: print(f'  >>> {x.shape}')\n",
    "        \n",
    "        if VERBOSE: print(f'  >> batch', end=' ')\n",
    "        x = self.bnorm1(x)\n",
    "        if VERBOSE: print(f'  >>> {x.shape}')\n",
    "            \n",
    "        if VERBOSE: print(f'  >> relu', end=' ')\n",
    "        x = nn.ReLU()(x)\n",
    "        if VERBOSE: print(f'  >>> {x.shape}')\n",
    "        \n",
    "        \n",
    "        \n",
    "        if VERBOSE: print(f'  >> conv2', end=' ')\n",
    "        x = self.conv2(x)\n",
    "        if VERBOSE: print(f'  >>> {x.shape}')\n",
    "        \n",
    "        if VERBOSE: print(f'  >> batch', end=' ')\n",
    "        x = self.bnorm2(x)\n",
    "        if VERBOSE: print(f'  >>> {x.shape}')\n",
    "        \n",
    "        if VERBOSE: print(f'  >> relu', end=' ')\n",
    "        x = nn.ReLU()(x)\n",
    "        if VERBOSE: print(f'  >>> {x.shape}')\n",
    "        \n",
    "        \n",
    "        if VERBOSE: print(f'  >> conv3', end=' ')\n",
    "        x = self.conv3(x)\n",
    "        if VERBOSE: print(f'  >>> {x.shape}')\n",
    "        \n",
    "        if VERBOSE: print(f'  >> batch', end=' ')\n",
    "        x = self.bnorm3(x)\n",
    "        if VERBOSE: print(f'  >>> {x.shape}')\n",
    "        \n",
    "        if VERBOSE: print(f'  >> relu', end=' ')\n",
    "        x = nn.ReLU()(x)\n",
    "        if VERBOSE: print(f'  >>> {x.shape}')\n",
    "        \n",
    "        \n",
    "\n",
    "        # Flatten the output and pass it through the fully connected layers\n",
    "        if VERBOSE: print(f'  >> flatten', end=' ')\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if VERBOSE: print(f'  >>> {x.shape}')\n",
    "        \n",
    "        if VERBOSE: print(f'  >> fc1', end=' ')\n",
    "        x = self.fc1(x)\n",
    "        if VERBOSE: print(f'  >>> {x.shape}')\n",
    "        \n",
    "        if VERBOSE: print(f'  >> relu', end=' ')\n",
    "        x = nn.ReLU()(x)\n",
    "        if VERBOSE: print(f'  >>> {x.shape}')\n",
    "\n",
    "        # Compute the policy and value outputs\n",
    "        if VERBOSE: print(f'  >> compute output', end=' ')\n",
    "        policy = nn.Softmax(1)(self.fc_policy(x))        \n",
    "        value = self.fc_value(x)\n",
    "        # print(value.shape)\n",
    "        \n",
    "        if VERBOSE: print(f'  >>> policy: {policy.shape}', end=' ')\n",
    "        if VERBOSE: print(f'  >>> value: {value.shape}')\n",
    "\n",
    "        if VERBOSE: print(f'>> forward complete >>>>>>>>>>>>>>>>')\n",
    "        return policy, value\n",
    "\n",
    "    def set_verbose(self, verbose):\n",
    "        self.VERBOSE = verbose\n",
    "        return\n",
    "    \n",
    "\n",
    "model = PolicyValueNetwork(kernel=3, padding=1)\n",
    "model.set_verbose(True)\n",
    "\n",
    "data = torch.load('board_data.pt')\n",
    "data = data[:512].reshape(-1,1,9,9)\n",
    "model.forward(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess ::::::::::::::::::::\n",
      ":::  Splitting :::\n",
      "::: :::  before splitting: (608160, 1, 9, 9) (608160, 2)\n",
      "::: :::  after splitting: (486528, 1, 9, 9) (121632, 1, 9, 9) (486528, 2) (121632, 2)\n",
      ":::  Batch processing :::\n",
      "::: :::  train loader batch size: 2048\n",
      "::: :::  train loader iters: 238\n"
     ]
    }
   ],
   "source": [
    "#TODO: check is pass move is accounted for\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "# get data\n",
    "inputs = torch.load('board_data.pt')\n",
    "# print(inputs.dtype)\n",
    "inputs = inputs.reshape(-1,1,9,9)\n",
    "\n",
    "plabels = torch.load('plabels.pt')\n",
    "vlabels = torch.load('vlabels.pt')\n",
    "# print(vlabels.shape)\n",
    "labels = torch.cat((plabels, vlabels), dim=1)\n",
    "\n",
    "print('Preprocess ::::::::::::::::::::')\n",
    "print(':::  Splitting :::')\n",
    "print(f'::: :::  before splitting:', tuple(inputs.shape), tuple(labels.shape))\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2)\n",
    "print(f'::: :::  after splitting:', tuple(X_train.shape), tuple(X_test.shape), tuple(y_train.shape), tuple(y_test.shape) )\n",
    "\n",
    "# Create a TensorDataset from the data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create a DataLoader with a batch size of 32\n",
    "print(':::  Batch processing :::')\n",
    "batch_size = 1024*2\n",
    "iters = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(f'::: :::  train loader batch size: {batch_size}')\n",
    "print(f'::: :::  train loader iters: {iters}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model.set_verbose(False)\n",
    "\n",
    "\n",
    "# Criterions \n",
    "def pcriterion(outputs, labels):\n",
    "    return F.cross_entropy(outputs, labels.long())\n",
    "\n",
    "def vcriterion(outputs, labels):\n",
    "    return F.mse_loss(outputs, labels)\n",
    "\n",
    "def pvcriterion(outputs, labels, alpha=0.5):\n",
    "    poutputs, voutputs = outputs\n",
    "    plabels = labels[:,0]#.squeeze()\n",
    "    vlabels = labels[:,1].view((-1,1))\n",
    "    # print(vlabels)\n",
    "    \n",
    "    vcrit = vcriterion(voutputs, vlabels)\n",
    "    pcrit = pcriterion(poutputs, plabels)\n",
    "    \n",
    "    return vcrit + alpha * vcrit\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      ":::  Epoch 1 ::: batch_iter(951/951) ::: loss:0.2152 ::: time: 551.47s ::: iter_time: 0.2041s\n",
      ":::  Epoch 2 ::: batch_iter(902/951) ::: loss:0.2160 ::: time: 460.60s ::: iter_time: 0.4842s\r"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print('\\nTraining ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::')\n",
    "EPOCHS = 2\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        iter_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # print(outputs[0].dtype)\n",
    "        # print(outputs[1].dtype)\n",
    "        # print(labels[0].dtype)\n",
    "        # print(labels[1].dtype)\n",
    "        loss = pvcriterion(outputs, labels)\n",
    "        # assert False\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if i%50 == 0:\n",
    "        print(f':::  Epoch {epoch+1} ::: batch_iter({i+1}/{iters}) ::: loss:{loss.item():.4f} ::: time: {time.time()-start_time:.2f}s ::: iter_time: {time.time()-iter_time:.4f}s', end='\\r')\n",
    "    print('\\n', end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5896eaaea9cfda5f2dfd8f6cd191f6f35a93afed143d79e2a6afd278becd1866"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
